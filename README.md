
# ğŸ“˜ SoftSuave AI Assistant â€“  RAG Chatbot

## ğŸš€ Overview

SoftSuave AI Assistant is a **Section-Aware Retrieval-Augmented Generation (RAG) Chatbot** designed to intelligently answer questions from structured company documents like an Employee Handbook.

Unlike basic document chatbots, this system:

- Uses **section-aware chunking**
- Validates follow-up questions
- Prevents repeated follow-ups
- Handles structured yes/no conversational flow
- Retrieves precise answers from relevant document sections only

We implemented a Section-Aware RAG pipeline where documents are chunked using regex-based section detection combined with recursive semantic splitting. Chunks are embedded using OpenAI embeddings and stored in ChromaDB. On query, we retrieve top candidates via similarity search, rerank them using a cross-encoder model, and pass the refined context to the LLM for grounded answer generation with validated follow-up control.

---

## ğŸ§  Architecture Overview

```
User Question
     â†“
Intent Classification
     â†“
Query Rewriting
     â†“
Vector Search (Chroma)
     â†“
Re-ranking
     â†“
LLM Answer Generation
     â†“
Follow-up Validation
     â†“
Response
```

---

## ğŸ›  Tech Stack

- **Backend**: FastAPI
- **Vector Database**: ChromaDB
- **Embeddings**: OpenAI Embeddings
- **LLM**: GPT (via LangChain)
- **Document Processing**: Section-based + Hybrid Chunking
- **Frontend**: HTML, CSS, JS (Chat UI)

---

## ğŸ”¥ Key Features

### âœ… 1. Section-Aware Chunking

- Splits document by headings (e.g., `1.1 OUR VALUES`)
- Combines header + content
- Applies secondary semantic splitting for large sections
- Ensures small sections (like Mission â€“ 3 lines) are retrievable

---

### âœ… 2. Retrieval-Augmented Generation (RAG)

- Uses vector similarity search
- Re-ranks top results
- LLM answers strictly from retrieved context

---

### âœ… 3. Intelligent Follow-up System

- Generates at most one follow-up question
- Validates if section exists in metadata
- Prevents repeated follow-up questions
- Handles **YES / NO** conversational flow

---

### âœ… 4. Intent Classification Layer

Classifies messages into:

- Greeting
- Small Talk
- New Question
- Follow-up Reply

This prevents accidental follow-up triggering.

---

### âœ… 5. Repeat Follow-up Prevention

System checks conversation history and blocks previously asked follow-up questions.

---

## ğŸ“‚ Project Structure

```
app/
 â”œâ”€â”€ api/
 â”‚    â””â”€â”€ routes/
 â”œâ”€â”€ services/
 â”‚    â”œâ”€â”€ chat_service.py
 â”‚    â”œâ”€â”€ reranker_service.py
 â”œâ”€â”€ vectorstore/
 â”‚    â””â”€â”€ chroma_db.py
 â”œâ”€â”€ core/
 â”‚    â””â”€â”€ llm.py
 â”œâ”€â”€ templates/
 â”œâ”€â”€ static/
 â””â”€â”€ main.py
```

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Document Upload

Admin uploads Employee Handbook PDF.

### 2ï¸âƒ£ Section-Based Chunking

Document is split into:

- Section heading
- Section content
- Hybrid semantic chunks if large

Each chunk stores:

```
{
  "section":"1.3 OUR MISSION",
  "source":"handbook"
}
```

---

### 3ï¸âƒ£ Embedding & Storage

Chunks are embedded using OpenAI embeddings and stored in ChromaDB.

---

### 4ï¸âƒ£ User Interaction Flow

- User asks question
- Query rewritten for retrieval optimization
- Similarity search performed
- Top chunks re-ranked
- LLM generates answer from context only
- Optional follow-up validated
- Response returned

---

# ğŸ”§ Technical Implementation Details

## ğŸ“Œ 1. Chunking Strategy

We implemented a **Hybrid Section-Aware Chunking** approach using:

- `re.split()` (Regex-based section detection)
- `RecursiveCharacterTextSplitter` from LangChain

Small sections (e.g., Mission â€“ 3 lines) remain intact.

Large sections are semantically split into 800-token chunks with overlap.

---

## ğŸ“Œ 2. Embedding Model

We used:

- `OpenAIEmbeddings`
- Model: `text-embedding-3-small` (or equivalent)

Each chunk is converted into a dense vector representation and stored in ChromaDB.

---

## ğŸ“Œ 3. Vector Database

We used:

- `Chroma` (langchain_chroma)
- Persistent storage via `persist_directory`

Key functions:

- `add_documents()`
- `similarity_search_with_score()`

---

## ğŸ“Œ 4. Reranking Model

We applied cross-encoder-based reranking using:

- Sentence Transformers CrossEncoder
- Example model: `cross-encoder/ms-marco-MiniLM-L-6-v2`

This improves retrieval precision before passing context to the LLM.

---

## ğŸ“Œ 5. LLM

We used:

- OpenAI Chat Model (via LangChain)
- Controlled prompt engineering
- Strict context-only answering

## ğŸ¯ Why This Project is Advanced

This is not a basic chatbot.

It includes:

- Section-aware retrieval
- Metadata validation
- Controlled follow-up logic
- Context re-retrieval for follow-up answers
- Hybrid chunking strategy
- Repeat prevention logic
- Structured conversational control

---

## ğŸ§ª Example Flow

**User:** What awards does SoftSuave offer?

**Bot:** Long Service Award...

**Bot:** What benefits do employees receive after completing service?

**User:** Yes

**Bot:** (Retrieves benefits section and answers)

System prevents:

- Repeated follow-ups
- Invalid sections
- Hallucinated answers

---

## ğŸ“Œ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```
git clone <repo-url>
cd project-folder
```

### 2ï¸âƒ£ Install Dependencies

```
pip install-r requirements.txt
```

### 3ï¸âƒ£ Add Environment Variables

Create `.env` file:

```
OPENAI_API_KEY=your_key_here
```

---

### 4ï¸âƒ£ Run Backend

```
uvicorn app.main:app--reload
```

Open:

```
http://127.0.0.1:8000/chat
```

---

## ğŸ§¹ Reset Vector Database (If Chunking Changes)

If chunking logic changes:

Delete Chroma directory:

```
Remove-Item-Recurse-Forcedata\chroma
```

Then re-upload documents.
